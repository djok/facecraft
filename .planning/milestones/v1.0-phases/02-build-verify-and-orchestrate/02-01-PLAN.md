---
phase: 02-build-verify-and-orchestrate
plan: 01
type: execute
wave: 1
depends_on: []
files_modified: []
autonomous: false

must_haves:
  truths:
    - "CPU image starts successfully with no network access (all models bundled)"
    - "CPU image runs as non-root user appuser"
    - "CPU image contains CPU-only PyTorch (torch.version.cuda is None)"
    - "GPU image builds successfully from hardened Dockerfile"
    - "GPU image runs as non-root user appuser"
  artifacts: []
  key_links:
    - from: "docker/Dockerfile.cpu"
      to: "djok/facecraft:cpu"
      via: "docker build"
      pattern: "docker build.*-t djok/facecraft:cpu"
    - from: "docker/Dockerfile.gpu"
      to: "djok/facecraft:gpu"
      via: "docker build"
      pattern: "docker build.*-t djok/facecraft:gpu"
---

<objective>
Build both Docker images (CPU and GPU) from the hardened Dockerfiles and run the full verification checklist: air-gap self-containment, non-root execution, CPU-only PyTorch, and image metadata.

Purpose: Phase 1 hardened the Dockerfiles statically. This plan proves the images actually build correctly and meet all runtime requirements. Without building and verifying, the Phase 1 changes are untested assumptions.
Output: Two locally-built Docker images (`djok/facecraft:cpu` and `djok/facecraft:gpu`) that pass all verification checks.
</objective>

<execution_context>
@/home/rosen/.claude/get-shit-done/workflows/execute-plan.md
@/home/rosen/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-dockerfile-hardening/01-01-SUMMARY.md
@.planning/phases/01-dockerfile-hardening/01-02-SUMMARY.md
@docker/Dockerfile.cpu
@docker/Dockerfile.gpu
</context>

<tasks>

<task type="auto">
  <name>Task 1: Build and verify CPU image</name>
  <files></files>
  <action>
Build the CPU Docker image from the hardened Dockerfile:

```bash
docker build -f docker/Dockerfile.cpu -t djok/facecraft:cpu .
```

This will take several minutes (model downloads ~600MB, dlib compilation). After a successful build, run the verification checklist:

1. **Air-gap test** — prove all models are bundled, no internet needed:
```bash
docker run --rm --network none djok/facecraft:cpu \
  python -c "print('Air-gap: container starts without internet')"
```
Expected: prints the message and exits 0.

2. **Non-root user test**:
```bash
docker run --rm djok/facecraft:cpu whoami
```
Expected output: `appuser`

3. **CPU-only PyTorch test** — proves CPU wheels were used (no CUDA bundled):
```bash
docker run --rm djok/facecraft:cpu \
  python -c "import torch; print('cuda:', torch.version.cuda); assert torch.version.cuda is None, 'CUDA should be None for CPU image'"
```
Expected output: `cuda: None`

4. **OCI labels test**:
```bash
docker inspect --format '{{json .Config.Labels}}' djok/facecraft:cpu | python -m json.tool
```
Expected: labels include `org.opencontainers.image.source`, `.version`, `.description`, `.licenses`.

5. **Image size check** (informational — record for STATE.md):
```bash
docker images djok/facecraft:cpu --format "{{.Size}}"
```
Record the actual size. Target was <8GB but this is informational, not a gate.

If any verification step fails, diagnose and fix the issue before proceeding. SHA256 checksum failures mean the model URL changed — recompute with `make update-checksums`. Build failures from pip likely mean a dependency conflict — check requirements.txt.
  </action>
  <verify>
All 4 verification commands exit with code 0:
- `docker run --rm --network none djok/facecraft:cpu python -c "print('ok')"` succeeds
- `docker run --rm djok/facecraft:cpu whoami` prints `appuser`
- `docker run --rm djok/facecraft:cpu python -c "import torch; assert torch.version.cuda is None"` succeeds
- `docker inspect` shows OCI labels present
  </verify>
  <done>CPU image djok/facecraft:cpu is built locally. Air-gap test passes. whoami returns appuser. torch.version.cuda is None. OCI labels are present.</done>
</task>

<task type="auto">
  <name>Task 2: Build and verify GPU image</name>
  <files></files>
  <action>
Build the GPU Docker image from the hardened Dockerfile:

```bash
docker build -f docker/Dockerfile.gpu -t djok/facecraft:gpu .
```

This will also take several minutes (CUDA base image is larger, dlib compilation against CUDA). After a successful build, run the verification checklist:

1. **Air-gap test** — prove all models are bundled:
```bash
docker run --rm --network none djok/facecraft:gpu \
  python -c "print('Air-gap: GPU container starts without internet')"
```
Expected: prints message and exits 0. Note: the GPU image can still run Python on a CPU-only host — it just cannot use CUDA at runtime.

2. **Non-root user test**:
```bash
docker run --rm djok/facecraft:gpu whoami
```
Expected output: `appuser`

3. **OCI labels test**:
```bash
docker inspect --format '{{json .Config.Labels}}' djok/facecraft:gpu | python -m json.tool
```
Expected: labels include `org.opencontainers.image.*` with GPU/CUDA variant description.

4. **NVIDIA env vars test**:
```bash
docker inspect --format '{{json .Config.Env}}' djok/facecraft:gpu
```
Expected: contains `NVIDIA_VISIBLE_DEVICES=all` and `NVIDIA_DRIVER_CAPABILITIES=compute,utility`.

5. **Image size check** (informational):
```bash
docker images djok/facecraft:gpu --format "{{.Size}}"
```
Record the actual size. Target was <12GB but this is informational, not a gate.

If build fails on the CUDA builder stage, this is likely a dependency or package version issue. The GPU image does NOT require a host GPU to build — CUDA base images are just Ubuntu images with CUDA libs installed.
  </action>
  <verify>
All verification commands exit with code 0:
- `docker run --rm --network none djok/facecraft:gpu python -c "print('ok')"` succeeds
- `docker run --rm djok/facecraft:gpu whoami` prints `appuser`
- `docker inspect` shows OCI labels and NVIDIA env vars present
  </verify>
  <done>GPU image djok/facecraft:gpu is built locally. Air-gap test passes. whoami returns appuser. OCI labels and NVIDIA env vars are present.</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 3: Verify build results</name>
  <files></files>
  <action>
**What was built:** Both Docker images (CPU and GPU) built and verified with automated checks: air-gap self-containment, non-root user, CPU-only PyTorch, OCI labels, NVIDIA env vars. Image sizes recorded.

**How to verify:**
1. Review the build output and verification results from Tasks 1 and 2
2. Check that both images appear in `docker images | grep facecraft`
3. Confirm image sizes are reasonable (CPU target less than 8GB, GPU target less than 12GB — but not a hard gate)
4. If you have a GPU available: optionally test `docker run --rm --gpus all djok/facecraft:gpu python -c "import torch; print(torch.cuda.is_available())"` to verify GPU passthrough

**Resume signal:** Type "approved" to proceed to docker-compose.yml creation, or describe any issues.
  </action>
  <verify>User approves the build results.</verify>
  <done>Both Docker images verified and approved by user. Ready for docker-compose.yml creation.</done>
</task>

</tasks>

<verification>
Phase 2 success criteria addressed by this plan:
- SC-1: `docker run --network none djok/facecraft:cpu` starts successfully (Task 1, verification 1)
- SC-2: `docker run --rm djok/facecraft:cpu whoami` prints `appuser` (Task 1, verification 2)
</verification>

<success_criteria>
- Both `djok/facecraft:cpu` and `djok/facecraft:gpu` images exist locally
- CPU image passes: air-gap, whoami=appuser, torch.version.cuda=None, OCI labels
- GPU image passes: air-gap, whoami=appuser, OCI labels, NVIDIA env vars
- Image sizes recorded for project state tracking
</success_criteria>

<output>
After completion, create `.planning/phases/02-build-verify-and-orchestrate/02-01-SUMMARY.md`
</output>
